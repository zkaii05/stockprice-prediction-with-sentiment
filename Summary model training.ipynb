{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d62d434-7494-4b85-9072-632904d8d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "basic requirements for project\n",
    "includes file reading\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "data_own_model = pd.read_csv(r\"C:\\Users\\Zhun Kai\\Downloads\\archive\\news_summary.csv\", encoding='latin1')\n",
    "\n",
    "data_own_model['text'] = data_own_model['text'].fillna('').astype(str)\n",
    "data_own_model['headlines'] = data_own_model['ctext'].fillna('').astype(str)\n",
    "\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b247ac3b-c040-4cd8-92df-23677f012019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data: 100%|████████████████████████████████████████████████████████████| 4514/4514 [00:08<00:00, 553.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1: 0.6662\n",
      "ROUGE-2: 0.3505\n",
      "ROUGE-L: 0.6060\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Custom version 1, TF-IDF alone\n",
    "\"\"\"\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    # Remove unnecessary punctuation (but keep periods)\n",
    "    text = re.sub(r'[^\\w\\s\\.\\-]', '', text)  # Keeps periods, hyphens, and alphanumeric characters\n",
    "    sentences = sent_tokenize(text)  # Tokenize into sentences\n",
    "    sw = set(stopwords.words('english'))  # Load stopwords\n",
    "    filtered_sentences = [\n",
    "        \" \".join(w for w in sent.split() if w not in sw)\n",
    "        for sent in sentences\n",
    "    ]\n",
    "    return filtered_sentences\n",
    "\n",
    "def score_sentences(sentences):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    # Return the sum of TF-IDF scores across words for each sentence\n",
    "    return tfidf_matrix.sum(axis=1).A1  # Sum scores across words for each sentence\n",
    "\n",
    "# Extract top sentences based on scores\n",
    "def extract_summary(sentences, sentence_scores, n=3):\n",
    "    top_indices = np.argsort(sentence_scores)[-n:][::-1]  # Top n scores\n",
    "    return ' '.join(sentences[i] for i in top_indices)\n",
    "\n",
    "# Summarize document\n",
    "def summarize_document(document, n=3):\n",
    "    sentences = preprocess_text(document)\n",
    "    sentence_scores = score_sentences(sentences)\n",
    "    return extract_summary(sentences, sentence_scores, n)\n",
    "\n",
    "# ROUGE evaluation\n",
    "def evaluate_summaries(reference_summaries, generated_summaries):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    results = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    for ref, gen in zip(reference_summaries, generated_summaries):\n",
    "        scores = scorer.score(ref, gen)\n",
    "        results['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "        results['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "        results['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    # Compute averages\n",
    "    avg_scores = {metric: sum(values) / len(values) for metric, values in results.items()}\n",
    "    return avg_scores\n",
    "\n",
    "# Process dataset and summarize\n",
    "def process_dataset(data):\n",
    "    generated_summaries = []\n",
    "    reference_summaries = []\n",
    "\n",
    "    # Loop through each row in the dataset\n",
    "    for _, row in tqdm(data.iterrows(), total=len(data), desc=\"Processing Data\"):\n",
    "        text = row['text']\n",
    "        reference = row['headlines']\n",
    "\n",
    "        # Generate summary for the current row\n",
    "        generated_summary = summarize_document(text, n=2)  # Adjust the number of sentences if needed\n",
    "        \n",
    "        # Store results\n",
    "        generated_summaries.append(generated_summary)\n",
    "        reference_summaries.append(reference)\n",
    "\n",
    "    return generated_summaries, reference_summaries\n",
    "\n",
    "# Process the dataset\n",
    "generated_summaries, reference_summaries = process_dataset(data_own_model)\n",
    "\n",
    "# Evaluate the generated summaries with ROUGE\n",
    "rouge_scores = evaluate_summaries(data_own_model['text'], generated_summaries)\n",
    "\n",
    "# Print the average ROUGE scores\n",
    "print(\"Average ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22911d79-abed-42e9-9b42-54228ebcecb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data: 100%|████████████████████████████████████████████████████████████| 4514/4514 [00:09<00:00, 472.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1: 0.6662\n",
      "ROUGE-2: 0.3490\n",
      "ROUGE-L: 0.5481\n",
      "Summarized data saved to C:\\Users\\Zhun Kai\\Downloads\\summarized_news.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Custom version 2, TF-IDF + cosine similarity\"\"\"\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'[^\\w\\s\\.\\-]', '', text)  # Keeps periods, hyphens, and alphanumeric characters\n",
    "    sentences = sent_tokenize(text)  # Tokenize into sentences\n",
    "    sw = set(stopwords.words('english'))  # Load stopwords\n",
    "    filtered_sentences = [\n",
    "        \" \".join(w for w in sent.split() if w not in sw)\n",
    "        for sent in sentences\n",
    "    ]\n",
    "    return filtered_sentences\n",
    "\n",
    "# Score sentences based on TF-IDF\n",
    "def score_sentences(sentences):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    sentence_scores = tfidf_matrix.sum(axis=1).A1  # Sum scores across words for each sentence\n",
    "    return tfidf_matrix, sentence_scores\n",
    "\n",
    "# Extract top sentences based on scores and similarity\n",
    "def extract_summary(sentences, sentence_scores, tfidf_matrix, n=3):\n",
    "    top_indices = np.argsort(sentence_scores)[-n:][::-1]\n",
    "    selected_sentences = [sentences[i] for i in top_indices]\n",
    "    \n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "    avg_similarities = np.mean(cosine_sim[top_indices], axis=1)\n",
    "    refined_indices = np.argsort(avg_similarities)[-n:][::-1]\n",
    "    \n",
    "    return ' '.join(selected_sentences[i] for i in refined_indices)\n",
    "\n",
    "# Summarize document\n",
    "def summarize_document(document, n=3):\n",
    "    sentences = preprocess_text(document)\n",
    "    tfidf_matrix, sentence_scores = score_sentences(sentences)\n",
    "    summary = extract_summary(sentences, sentence_scores, tfidf_matrix, n)\n",
    "    return summary\n",
    "\n",
    "# ROUGE evaluation\n",
    "def evaluate_summaries(reference_summaries, generated_summaries):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    results = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    for ref, gen in zip(reference_summaries, generated_summaries):\n",
    "        scores = scorer.score(ref, gen)\n",
    "        results['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "        results['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "        results['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    avg_scores = {metric: sum(values) / len(values) for metric, values in results.items()}\n",
    "    return avg_scores\n",
    "\n",
    "# Process dataset and summarize with progress bar\n",
    "def process_dataset(data):\n",
    "    generated_summaries = []\n",
    "\n",
    "    for _, row in tqdm(data.iterrows(), total=len(data), desc=\"Processing Data\"):\n",
    "        text = row['text']\n",
    "        # Generate summary for the current row\n",
    "        generated_summary = summarize_document(text, n=2)  # Adjust the number of sentences if needed\n",
    "        generated_summaries.append(generated_summary)\n",
    "\n",
    "    return generated_summaries\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Zhun Kai\\Downloads\\archive\\news_summary.csv\"\n",
    "data_own_model = pd.read_csv(data_path, encoding='latin1')\n",
    "data_own_model = data_own_model[['text', 'headlines']].dropna()\n",
    "\n",
    "# Process the dataset and add summaries to a new column\n",
    "data_own_model['generated_summary'] = process_dataset(data_own_model)\n",
    "\n",
    "# Evaluate summaries using ROUGE\n",
    "rouge_scores = evaluate_summaries(data_own_model['text'], data_own_model['generated_summary'])\n",
    "\n",
    "# Print ROUGE scores\n",
    "print(\"Average ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "\n",
    "# Save the updated dataset to a CSV file\n",
    "output_path = r\"C:\\Users\\Zhun Kai\\Downloads\\summarized_news.csv\"\n",
    "data_own_model.to_csv(output_path, index=False)\n",
    "print(f\"Summarized data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a87b7-d6c2-43ba-b2ad-7109fa422893",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pretrained model\n",
    "\"\"\"\n",
    "\n",
    "# Load the summarization model\n",
    "pre_trained_summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device=0)  # -1 for CPU, 0 for GPU\n",
    "\n",
    "# Function to evaluate ROUGE scores\n",
    "def evaluate_summaries(reference_summaries, generated_summaries):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    results = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    for ref, gen in zip(reference_summaries, generated_summaries):\n",
    "        scores = scorer.score(ref, gen)\n",
    "        results['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "        results['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "        results['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    # Compute averages\n",
    "    avg_scores = {metric: sum(values) / len(values) for metric, values in results.items()}\n",
    "    return avg_scores\n",
    "\n",
    "# Process dataset and generate summaries with progress bar\n",
    "def process_dataset(data):\n",
    "    generated_summaries = []\n",
    "    reference_summaries = []\n",
    "\n",
    "    # Use tqdm to add a progress bar for the loop\n",
    "    for _, row in tqdm(data.iterrows(), total=len(data), desc=\"Processing Data\"):\n",
    "        text = row['text']\n",
    "        reference = row['headlines']\n",
    "\n",
    "        # Generate summary for the current row using the pretrained model\n",
    "        pre_trained_model_summary = pre_trained_summarizer(text, max_length=150, min_length=50, do_sample=False)\n",
    "        \n",
    "        # Extract the generated summary\n",
    "        generated_summary = pre_trained_model_summary[0]['summary_text']\n",
    "        \n",
    "        # Store results\n",
    "        generated_summaries.append(generated_summary)\n",
    "        reference_summaries.append(reference)\n",
    "\n",
    "    return generated_summaries, reference_summaries\n",
    "\n",
    "# Process the dataset and get summaries\n",
    "generated_summaries, reference_summaries = process_dataset(data_own_model)\n",
    "\n",
    "# Evaluate the generated summaries with ROUGE against headlines\n",
    "rouge_scores_headlines = evaluate_summaries(data_own_model['headlines'], generated_summaries)\n",
    "print(\"ROUGE Scores (Generated Summary vs Headline):\")\n",
    "print(f\"ROUGE-1: {rouge_scores_headlines['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge_scores_headlines['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_scores_headlines['rougeL']:.4f}\")\n",
    "\n",
    "# Evaluate the generated summaries with ROUGE against original text\n",
    "rouge_scores_text = evaluate_summaries(data_own_model['text'], generated_summaries)\n",
    "print(\"\\nROUGE Scores (Generated Summary vs Original Text):\")\n",
    "print(f\"ROUGE-1: {rouge_scores_text['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge_scores_text['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_scores_text['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ddfb6c-01e0-4920-bf5a-8c66abe40eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCurrent hyperparameter in custom model includes:\\n1. TF-IDF Vectorizer:\\n    a. max_features: The number of features to keep based on frequency.\\n    b. stop_words: To remove common words that do not carry much information.\\n    c. ngram_range: Consider adjusting the range of n-grams to include bigrams or trigrams.\\n    d. min_df and max_df: These control the inclusion of words that appear too rarely or too frequently across the corpus.\\n2. Summarization Model (Custom) Parameters:\\n    a. n: The number of sentences in the summary. This is a key hyperparameter to adjust based on the length of the summary you want.\\n3. Cosine Similarity:\\n    a. weight of sentence similarity during the refinement of the summary.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Current hyperparameter in custom model includes:\n",
    "1. TF-IDF Vectorizer:\n",
    "    a. max_features: The number of features to keep based on frequency.\n",
    "    b. stop_words: To remove common words that do not carry much information.\n",
    "    c. ngram_range: Consider adjusting the range of n-grams to include bigrams or trigrams.\n",
    "    d. min_df and max_df: These control the inclusion of words that appear too rarely or too frequently across the corpus.\n",
    "2. Summarization Model (Custom) Parameters:\n",
    "    a. n: The number of sentences in the summary. This is a key hyperparameter to adjust based on the length of the summary you want.\n",
    "3. Cosine Similarity:\n",
    "    a. weight of sentence similarity during the refinement of the summary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db3d4d1f-cb29-46e0-a3c9-7c8b4731bba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 3611\n",
      "Validation Set: 451\n",
      "Test Set: 452\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "splitting data\n",
    "\"\"\"\n",
    "\n",
    "# Split into train (80%), validation (10%), and test (10%)\n",
    "train_data, temp_data = train_test_split(data_own_model, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check the sizes of each set\n",
    "print(\"Training Set:\", len(train_data))\n",
    "print(\"Validation Set:\", len(val_data))\n",
    "print(\"Test Set:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07077f7d-cabe-4b64-9eb3-14c0be895d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: max_features=500, ngram_range=(1, 1), stop_words=None\n",
      "Mean Cosine Similarity: 0.7361\n",
      "Testing parameters: max_features=500, ngram_range=(1, 1), stop_words=english\n",
      "Mean Cosine Similarity: 0.9184\n",
      "Testing parameters: max_features=500, ngram_range=(1, 2), stop_words=None\n",
      "Mean Cosine Similarity: 0.6576\n",
      "Testing parameters: max_features=500, ngram_range=(1, 2), stop_words=english\n",
      "Mean Cosine Similarity: 0.9177\n",
      "Testing parameters: max_features=1000, ngram_range=(1, 1), stop_words=None\n",
      "Mean Cosine Similarity: 0.7940\n",
      "Testing parameters: max_features=1000, ngram_range=(1, 1), stop_words=english\n",
      "Mean Cosine Similarity: 0.9199\n",
      "Testing parameters: max_features=1000, ngram_range=(1, 2), stop_words=None\n",
      "Mean Cosine Similarity: 0.7103\n",
      "Testing parameters: max_features=1000, ngram_range=(1, 2), stop_words=english\n",
      "Mean Cosine Similarity: 0.9199\n",
      "Testing parameters: max_features=2000, ngram_range=(1, 1), stop_words=None\n",
      "Mean Cosine Similarity: 0.8307\n",
      "Testing parameters: max_features=2000, ngram_range=(1, 1), stop_words=english\n",
      "Mean Cosine Similarity: 0.9211\n",
      "Testing parameters: max_features=2000, ngram_range=(1, 2), stop_words=None\n",
      "Mean Cosine Similarity: 0.7335\n",
      "Testing parameters: max_features=2000, ngram_range=(1, 2), stop_words=english\n",
      "Mean Cosine Similarity: 0.9216\n",
      "Best Parameters: {'max_features': 2000, 'ngram_range': (1, 2), 'stop_words': 'english'}\n",
      "Best Mean Cosine Similarity: 0.9215586629427377\n",
      "Best TF-IDF Vectorizer Configuration: {'max_features': 2000, 'ngram_range': (1, 2), 'stop_words': 'english'}\n",
      "Best Cosine Similarity: 0.9215586629427377\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "fine tuning TF-IDF\n",
    "\"\"\"\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Zhun Kai\\Downloads\\summarized_news.csv\"\n",
    "data_own_model_fine_tuning = pd.read_csv(data_path, encoding='latin1')\n",
    "data_own_model_fine_tuning = data_own_model_fine_tuning[['text', 'generated_summary']].dropna()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_texts, test_texts, train_summaries, test_summaries = train_test_split(\n",
    "    data_own_model_fine_tuning['text'], data_own_model_fine_tuning['generated_summary'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the parameter grid for TF-IDF\n",
    "param_grid = {\n",
    "    'max_features': [500, 1000, 2000],\n",
    "    'ngram_range': [(1, 1), (1, 2)],\n",
    "    'stop_words': [None, 'english']\n",
    "}\n",
    "\n",
    "# Function to fine-tune TF-IDF using cosine similarity\n",
    "def fine_tune_tfidf(train_texts, train_summaries, param_grid):\n",
    "    best_score = -1\n",
    "    best_params = None\n",
    "    best_vectorizer = None\n",
    "\n",
    "    for max_features in param_grid['max_features']:\n",
    "        for ngram_range in param_grid['ngram_range']:\n",
    "            for stop_words in param_grid['stop_words']:\n",
    "                print(f\"Testing parameters: max_features={max_features}, ngram_range={ngram_range}, stop_words={stop_words}\")\n",
    "                \n",
    "                # Initialize TfidfVectorizer with current parameters\n",
    "                tfidf_vectorizer = TfidfVectorizer(\n",
    "                    max_features=max_features,\n",
    "                    ngram_range=ngram_range,\n",
    "                    stop_words=stop_words\n",
    "                )\n",
    "                \n",
    "                # Fit and transform the training texts and summaries\n",
    "                tfidf_train_texts = tfidf_vectorizer.fit_transform(train_texts)\n",
    "                tfidf_train_summaries = tfidf_vectorizer.transform(train_summaries)\n",
    "                \n",
    "                # Compute cosine similarities\n",
    "                similarities = cosine_similarity(tfidf_train_texts, tfidf_train_summaries)\n",
    "                \n",
    "                # Calculate the mean diagonal similarity (main diagonal shows input-output match)\n",
    "                mean_similarity = np.mean(np.diag(similarities))\n",
    "                print(f\"Mean Cosine Similarity: {mean_similarity:.4f}\")\n",
    "                \n",
    "                # Update best parameters if the current mean similarity is higher\n",
    "                if mean_similarity > best_score:\n",
    "                    best_score = mean_similarity\n",
    "                    best_params = {'max_features': max_features, 'ngram_range': ngram_range, 'stop_words': stop_words}\n",
    "                    best_vectorizer = tfidf_vectorizer\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Mean Cosine Similarity:\", best_score)\n",
    "    return best_vectorizer, best_params, best_score\n",
    "\n",
    "# Fine-tune TF-IDF using the function\n",
    "best_vectorizer, best_params, best_score = fine_tune_tfidf(train_texts, train_summaries, param_grid)\n",
    "\n",
    "# Display the best configuration\n",
    "print(\"Best TF-IDF Vectorizer Configuration:\", best_params)\n",
    "print(\"Best Cosine Similarity:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "607208ab-5eb9-42fd-baf7-c017065147c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cosine Similarity on Test Data: 0.9217\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "checking feature against test data to confirm reliability\n",
    "\"\"\"\n",
    "\n",
    "# Transform the test data using the best TF-IDF vectorizer\n",
    "tfidf_test_texts = best_vectorizer.transform(test_texts)\n",
    "tfidf_test_summaries = best_vectorizer.transform(test_summaries)\n",
    "\n",
    "# Compute cosine similarities on the test set\n",
    "test_similarities = cosine_similarity(tfidf_test_texts, tfidf_test_summaries)\n",
    "\n",
    "# Calculate the mean diagonal similarity for the test set\n",
    "mean_test_similarity = np.mean(np.diag(test_similarities))\n",
    "print(f\"Mean Cosine Similarity on Test Data: {mean_test_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4234dbb6-1182-4b38-8635-4d0471196b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Zhun\n",
      "[nltk_data]     Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Zhun\n",
      "[nltk_data]     Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Zhun\n",
      "[nltk_data]     Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Generating Summaries: 100%|███████████████████████████████████████████████████████| 4514/4514 [00:08<00:00, 548.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores:\n",
      "ROUGE-1: 0.7436\n",
      "ROUGE-2: 0.3822\n",
      "ROUGE-L: 0.6386\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Custom model version 3, with fine tuned TF-IDF variables\"\"\"\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    # Remove unnecessary punctuation (but keep periods)\n",
    "    text = re.sub(r'[^\\w\\s\\.\\-]', '', text)  # Keeps periods, hyphens, and alphanumeric characters\n",
    "    sentences = sent_tokenize(text)  # Tokenize into sentences\n",
    "    sw = set(stopwords.words('english'))  # Load stopwords\n",
    "    filtered_sentences = [\n",
    "        \" \".join(w for w in sent.split() if w not in sw)\n",
    "        for sent in sentences\n",
    "    ]\n",
    "    return filtered_sentences\n",
    "\n",
    "def score_sentences(sentences):\n",
    "    # Use the tuned TF-IDF parameters\n",
    "    vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 2), stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    # Return the sum of TF-IDF scores across words for each sentence\n",
    "    return tfidf_matrix.sum(axis=1).A1  # Sum scores across words for each sentence\n",
    "\n",
    "# Extract top sentences based on scores\n",
    "def extract_summary(sentences, sentence_scores, n=3):\n",
    "    top_indices = np.argsort(sentence_scores)[-n:][::-1]  # Top n scores\n",
    "    return ' '.join(sentences[i] for i in top_indices)\n",
    "\n",
    "# Summarize document\n",
    "def summarize_document(document, n=3):\n",
    "    sentences = preprocess_text(document)\n",
    "    sentence_scores = score_sentences(sentences)\n",
    "    return extract_summary(sentences, sentence_scores, n)\n",
    "\n",
    "# ROUGE evaluation\n",
    "def evaluate_summaries(reference_summaries, generated_summaries):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    results = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    for ref, gen in zip(reference_summaries, generated_summaries):\n",
    "        scores = scorer.score(ref, gen)\n",
    "        results['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "        results['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "        results['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    # Compute averages\n",
    "    avg_scores = {metric: sum(values) / len(values) for metric, values in results.items()}\n",
    "    return avg_scores\n",
    "\n",
    "# Process dataset and summarize\n",
    "def process_dataset(data):\n",
    "    data['generated_summary'] = None  # Initialize the new column for summaries\n",
    "\n",
    "    for index, row in tqdm(data.iterrows(), total=len(data), desc=\"Generating Summaries\"):\n",
    "        text = row['text']\n",
    "        generated_summary = summarize_document(text)  # Generate the summary\n",
    "        data.at[index, 'generated_summary'] = generated_summary  # Save it in the new column\n",
    "\n",
    "    return data\n",
    "\n",
    "# Process the dataset\n",
    "data_own_model = process_dataset(data_own_model)\n",
    "\n",
    "# Evaluate the generated summaries with ROUGE\n",
    "rouge_scores = evaluate_summaries(data_own_model['text'], data_own_model['generated_summary'])\n",
    "\n",
    "# Print the average ROUGE scores\n",
    "print(\"Average ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "395e0cd5-7e7c-4c07-8187-533456fdc308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Zhun\n",
      "[nltk_data]     Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Zhun\n",
      "[nltk_data]     Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorizer loaded successfully.\n",
      "Generating summaries for the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries: 100%|██████████████████████████████████████████████████████| 4514/4514 [00:04<00:00, 1125.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized data saved to summarized_news.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "applying custom model on target data, checking reliability\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import joblib\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# File paths\n",
    "TFIDF_VECTOR_PATH = \"tfidf_vectorizer.joblib\"\n",
    "SUMMARIZED_DATA_PATH = \"summarized_news.csv\"\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s\\.\\-]', '', text)  # Remove unnecessary punctuation\n",
    "    sentences = sent_tokenize(text)  # Split into sentences\n",
    "    sw = set(stopwords.words('english'))  # Load stopwords\n",
    "    filtered_sentences = [\n",
    "        \" \".join(w for w in sent.split() if w not in sw) for sent in sentences\n",
    "    ]\n",
    "    return filtered_sentences\n",
    "\n",
    "# Train and save TF-IDF vectorizer\n",
    "def train_and_save_vectorizer(documents):\n",
    "    vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 2), stop_words='english')\n",
    "    vectorizer.fit(documents)\n",
    "    joblib.dump(vectorizer, TFIDF_VECTOR_PATH)\n",
    "    print(f\"TF-IDF vectorizer trained and saved to {TFIDF_VECTOR_PATH}\")\n",
    "    return vectorizer\n",
    "\n",
    "# Load the saved TF-IDF vectorizer\n",
    "def load_vectorizer(path):\n",
    "    return joblib.load(path)\n",
    "\n",
    "# Score sentences using TF-IDF\n",
    "def score_sentences(sentences, vectorizer):\n",
    "    tfidf_matrix = vectorizer.transform(sentences)\n",
    "    return tfidf_matrix.sum(axis=1).A1  # Sum TF-IDF scores across words for each sentence\n",
    "\n",
    "# Extract the top n sentences for the summary\n",
    "def extract_summary(sentences, sentence_scores, n=3):\n",
    "    top_indices = np.argsort(sentence_scores)[-n:][::-1]  # Get indices of top n sentences\n",
    "    return ' '.join(sentences[i] for i in top_indices)\n",
    "\n",
    "# Summarize a single document\n",
    "def summarize_document(document, vectorizer, n=3):\n",
    "    sentences = preprocess_text(document)\n",
    "    sentence_scores = score_sentences(sentences, vectorizer)\n",
    "    return extract_summary(sentences, sentence_scores, n)\n",
    "\n",
    "# Summarize a dataset\n",
    "def process_dataset(data, vectorizer):\n",
    "    data['generated_summary'] = None  # Initialize new column\n",
    "    for index, row in tqdm(data.iterrows(), total=len(data), desc=\"Generating Summaries\"):\n",
    "        text = row['text']\n",
    "        data.at[index, 'generated_summary'] = summarize_document(text, vectorizer)\n",
    "    return data\n",
    "\n",
    "# Main process\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your dataset\n",
    "    data_path = r\"C:\\Users\\Zhun Kai\\Downloads\\archive\\news_summary.csv\"\n",
    "    data = pd.read_csv(data_path, encoding=\"latin1\")[['text', 'headlines']].dropna()\n",
    "\n",
    "    # Check if the TF-IDF vectorizer is already trained\n",
    "    try:\n",
    "        vectorizer = load_vectorizer(TFIDF_VECTOR_PATH)\n",
    "        print(\"TF-IDF vectorizer loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No existing vectorizer found. Training a new one...\")\n",
    "        sample_texts = data['text'].tolist()  # Use dataset to train\n",
    "        vectorizer = train_and_save_vectorizer(sample_texts)\n",
    "\n",
    "    # Process the dataset and generate summaries\n",
    "    print(\"Generating summaries for the dataset...\")\n",
    "    summarized_data = process_dataset(data, vectorizer)\n",
    "\n",
    "    # Save the summarized data to CSV\n",
    "    summarized_data.to_csv(SUMMARIZED_DATA_PATH, index=False)\n",
    "    print(f\"Summarized data saved to {SUMMARIZED_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8bfeaea-cc55-4037-8467-77bea4bef88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorizer loaded successfully.\n",
      "Generating summaries for the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries: 100%|██████████████████████████████████████████████████████| 4514/4514 [00:04<00:00, 1121.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating ROUGE Scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Headlines: 100%|██████████████████████████████████████████████████████| 4514/4514 [00:03<00:00, 1337.54it/s]\n",
      "Evaluating Ctext: 100%|███████████████████████████████████████████████████████████| 4514/4514 [00:32<00:00, 138.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average ROUGE Scores (Generated Summary vs Headline):\n",
      "ROUGE-1: 0.2600\n",
      "ROUGE-2: 0.0840\n",
      "ROUGE-L: 0.2202\n",
      "\n",
      "Average ROUGE Scores (Generated Summary vs Original Text - ctext):\n",
      "ROUGE-1: 0.1845\n",
      "ROUGE-2: 0.0669\n",
      "ROUGE-L: 0.1342\n",
      "\n",
      "Summarized data saved to summarized_news.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "deploying custom data\n",
    "\"\"\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Main process\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your dataset\n",
    "    data_path = r\"C:\\Users\\Zhun Kai\\Downloads\\archive\\news_summary.csv\"\n",
    "    news_data = pd.read_csv(data_path, encoding=\"latin1\")\n",
    "\n",
    "    # Check if the TF-IDF vectorizer is already trained\n",
    "    try:\n",
    "        vectorizer = load_vectorizer(TFIDF_VECTOR_PATH)\n",
    "        print(\"TF-IDF vectorizer loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No existing vectorizer found. Training a new one...\")\n",
    "        sample_texts = news_data['text'].tolist()  # Use dataset to train\n",
    "        vectorizer = train_and_save_vectorizer(sample_texts)\n",
    "\n",
    "    # Process the dataset and generate summaries\n",
    "    print(\"Generating summaries for the dataset...\")\n",
    "    summarized_data = process_dataset(news_data, vectorizer)\n",
    "\n",
    "    # Fill missing values to prevent issues during evaluation\n",
    "    news_data['ctext'] = news_data['ctext'].fillna(\"\").astype(str)\n",
    "    summarized_data['generated_summary'] = summarized_data['generated_summary'].fillna(\"\").astype(str)\n",
    "\n",
    "    # Initialize progress bar for ROUGE evaluation\n",
    "    print(\"\\nEvaluating ROUGE Scores...\")\n",
    "    rouge_results_headline = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    rouge_results_ctext = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    # Evaluate summaries against 'headline'\n",
    "    for ref, gen in tqdm(zip(news_data['headlines'], summarized_data['generated_summary']), \n",
    "                         total=len(news_data), desc=\"Evaluating Headlines\"):\n",
    "        scores = evaluate_summaries([ref], [gen])\n",
    "        for key in rouge_results_headline:\n",
    "            rouge_results_headline[key].append(scores[key])\n",
    "\n",
    "    # Evaluate summaries against 'ctext'\n",
    "    for ref, gen in tqdm(zip(news_data['ctext'], summarized_data['generated_summary']), \n",
    "                         total=len(news_data), desc=\"Evaluating Ctext\"):\n",
    "        scores = evaluate_summaries([ref], [gen])\n",
    "        for key in rouge_results_ctext:\n",
    "            rouge_results_ctext[key].append(scores[key])\n",
    "\n",
    "    # Calculate and print average ROUGE scores\n",
    "    print(\"\\nAverage ROUGE Scores (Generated Summary vs Headline):\")\n",
    "    print(f\"ROUGE-1: {sum(rouge_results_headline['rouge1']) / len(rouge_results_headline['rouge1']):.4f}\")\n",
    "    print(f\"ROUGE-2: {sum(rouge_results_headline['rouge2']) / len(rouge_results_headline['rouge2']):.4f}\")\n",
    "    print(f\"ROUGE-L: {sum(rouge_results_headline['rougeL']) / len(rouge_results_headline['rougeL']):.4f}\")\n",
    "\n",
    "    print(\"\\nAverage ROUGE Scores (Generated Summary vs Original Text - ctext):\")\n",
    "    print(f\"ROUGE-1: {sum(rouge_results_ctext['rouge1']) / len(rouge_results_ctext['rouge1']):.4f}\")\n",
    "    print(f\"ROUGE-2: {sum(rouge_results_ctext['rouge2']) / len(rouge_results_ctext['rouge2']):.4f}\")\n",
    "    print(f\"ROUGE-L: {sum(rouge_results_ctext['rougeL']) / len(rouge_results_ctext['rougeL']):.4f}\")\n",
    "\n",
    "    # Save the summarized data to CSV\n",
    "    summarized_data.to_csv(SUMMARIZED_DATA_PATH, index=False)\n",
    "    print(f\"\\nSummarized data saved to {SUMMARIZED_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67beb23-ddb7-45c9-b854-07552e24e683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
